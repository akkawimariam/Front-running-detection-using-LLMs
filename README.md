# Front-Running Attack Detection using Transformer Models

This repository contains the implementation, datasets, and experiments for our research on detecting front-running attacks in Ethereum transactions using transformer-based models.

## ðŸ“Œ Overview
Front-running attacks (displacement, insertion, suppression) represent a critical threat in decentralized blockchain systems.  
This project explores transformer-based models (Llama, Gemma) for detecting and classifying such attacks.

## ðŸ›  Features
- Data preprocessing pipeline using **Alchemy** and **Chainstack RPC**.  
- Automatic labeling of transactions by attack type.  
- Training and evaluation of transformer models with varying input lengths (64, 128, 256).  
- Metrics: Accuracy, Precision, Recall, and F1-score.  
- Reproducible experiments with baseline comparisons.  

## ðŸ“‚ Repository Structure
â”œâ”€â”€ data/ # Datasets (or scripts to fetch them)
â”œâ”€â”€ models/ # Pretrained and trained model checkpoints
â”œâ”€â”€ src/ # Core source code
â”‚ â”œâ”€â”€ preprocessing/ # Data cleaning and labeling scripts
â”‚ â”œâ”€â”€ training/ # Training and evaluation code
â”‚ â””â”€â”€ utils/ # Helper functions
â”œâ”€â”€ results/ # Output metrics, logs, and plots
â””â”€â”€ README.md # Project documentation
